{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "838e0da7-1d5b-4e02-ad7c-28adbf57a686",
   "metadata": {},
   "source": [
    "## Import all the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f20c346-9e40-4cb6-aaba-f906092c8ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import Input\n",
    "from datetime import datetime, timedelta\n",
    "from tensorflow.keras.layers import (\n",
    "    GRU, Add, Attention, Dense, Embedding,\n",
    "    LayerNormalization, Reshape, StringLookup, TextVectorization,\n",
    ")\n",
    "# Reduce TensorFlow logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Enable eager execution\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "print(\"=============Tensorflow Version: \", tf.version.VERSION, \"=============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53261913-4d4e-4635-84f9-ade5c9ba8735",
   "metadata": {},
   "source": [
    "## Read & Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab69c388-e3ab-45a6-8720-790f6204bc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000 \n",
    "ATTENTION_DIM = 512\n",
    "WORD_EMBEDDING_DIM = 128\n",
    "\n",
    "# InceptionResNetV2 takes (299, 299, 3) image as inputs\n",
    "# and return features in (8, 8, 1536) shape\n",
    "FEATURE_EXTRACTOR = tf.keras.applications.inception_resnet_v2.InceptionResNetV2(\n",
    "    include_top=False, weights=\"imagenet\"\n",
    ")\n",
    "IMG_HEIGHT = 299\n",
    "IMG_WIDTH = 299\n",
    "IMG_CHANNELS = 3\n",
    "FEATURES_SHAPE = (8, 8, 1536)\n",
    "\n",
    "GCS_DIR = \"gs://asl-public/data/tensorflow_datasets/\"\n",
    "BUFFER_SIZE = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b48de0-98b9-4868-81ce-4593043482ba",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff388cd-e0c2-43b1-92f5-f5055bd60450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_label(example):\n",
    "    caption = example[\"captions\"][\"text\"][0]  # only the first caption per image\n",
    "    img = example[\"image\"]\n",
    "    img = tf.image.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
    "    img = img / 255\n",
    "    return {\"image_tensor\": img, \"caption\": caption}\n",
    "\n",
    "trainds = tfds.load(\"coco_captions\", split=\"train\", data_dir=GCS_DIR)\n",
    "\n",
    "trainds = trainds.map(\n",
    "    get_image_label, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").shuffle(BUFFER_SIZE)\n",
    "trainds = trainds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "def add_start_end_token(data):\n",
    "    start = tf.convert_to_tensor(\"<start>\")\n",
    "    end = tf.convert_to_tensor(\"<end>\")\n",
    "    data[\"caption\"] = tf.strings.join(\n",
    "        [start, data[\"caption\"], end], separator=\" \"\n",
    "    )\n",
    "    return data\n",
    "\n",
    "trainds = trainds.map(add_start_end_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a319a2c5-7a91-4bfb-9c54-123af82f2a6e",
   "metadata": {},
   "source": [
    "## Tokenize the captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f994e451-135f-4b43-8bef-3e7125dad2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take up to 10 min\n",
    "MAX_CAPTION_LEN = 64\n",
    "\n",
    "def standardize(inputs):\n",
    "    inputs = tf.strings.lower(inputs)\n",
    "    return tf.strings.regex_replace(\n",
    "        inputs, r\"[!\\\"#$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~]?\", \"\"\n",
    "    )\n",
    "\n",
    "tokenizer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    standardize=standardize,\n",
    "    output_sequence_length=MAX_CAPTION_LEN,\n",
    ")\n",
    "\n",
    "tokenizer.adapt(trainds.map(lambda x: x[\"caption\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07f7d1e-8b82-45d6-92e1-666e3a7d032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer([\"<start> This is a sentence <end>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ce7a3-2c2f-438a-a29f-90c06314ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_captions = []\n",
    "for d in trainds.take(5):\n",
    "    sample_captions.append(d[\"caption\"].numpy())\n",
    "\n",
    "print(\"=================Sample Captions=================\")\n",
    "for sample_caption in sample_captions:\n",
    "  print(sample_caption)\n",
    "print(\"==================================\")\n",
    "print(tokenizer(sample_captions))\n",
    "\n",
    "for wordid in tokenizer([sample_captions[0]])[0]:\n",
    "    print(tokenizer.get_vocabulary()[wordid], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135f1a09-6b4c-4081-92aa-5ebc06e3e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup table: Word -> Index\n",
    "word_to_index = StringLookup(\n",
    "    mask_token=\"\", vocabulary=tokenizer.get_vocabulary()\n",
    ")\n",
    "\n",
    "# Lookup table: Index -> Word\n",
    "index_to_word = StringLookup(\n",
    "    mask_token=\"\", vocabulary=tokenizer.get_vocabulary(), invert=True\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def create_ds_fn(data):\n",
    "    img_tensor = data[\"image_tensor\"]\n",
    "    caption = tokenizer(data[\"caption\"])\n",
    "\n",
    "    target = tf.roll(caption, -1, 0)\n",
    "    zeros = tf.zeros([1], dtype=tf.int64)\n",
    "    target = tf.concat((target[:-1], zeros), axis=-1)\n",
    "    return (img_tensor, caption), target\n",
    "\n",
    "batched_ds = (\n",
    "    trainds.map(create_ds_fn)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "for (img, caption), label in batched_ds.take(2):\n",
    "    print(f\"Image shape: {img.shape}\")\n",
    "    print(f\"Caption shape: {caption.shape}\")\n",
    "    print(f\"Label shape: {label.shape}\")\n",
    "    print(caption[0])\n",
    "    print(label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b35f0d-b090-4265-a5c7-82be74de533b",
   "metadata": {},
   "source": [
    "## Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599cad57-7d04-4d2e-851c-2f28cfa682c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================== Encoder =====================\n",
    "FEATURE_EXTRACTOR.trainable = False\n",
    "\n",
    "image_input = Input(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "image_features = FEATURE_EXTRACTOR(image_input)\n",
    "\n",
    "x = Reshape((FEATURES_SHAPE[0] * FEATURES_SHAPE[1], FEATURES_SHAPE[2]))(\n",
    "    image_features\n",
    ")\n",
    "encoder_output = Dense(ATTENTION_DIM, activation=\"relu\")(x)\n",
    "\n",
    "encoder = tf.keras.Model(inputs=image_input, outputs=encoder_output)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1ca3b6-de60-4bb9-b17d-bdbd997f99fd",
   "metadata": {},
   "source": [
    "## Caption Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd52a89b-6070-43e0-bb77-fb1e79b87102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================== Caption Decoder =====================\n",
    "word_input = Input(shape=(MAX_CAPTION_LEN,), name=\"words\")\n",
    "embed_x = Embedding(VOCAB_SIZE, ATTENTION_DIM)(word_input)\n",
    "\n",
    "decoder_gru = GRU(\n",
    "    ATTENTION_DIM,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    ")\n",
    "gru_output, gru_state = decoder_gru(embed_x)\n",
    "\n",
    "decoder_attention = Attention()\n",
    "context_vector = decoder_attention([gru_output, encoder_output])\n",
    "\n",
    "addition = Add()([gru_output, context_vector])\n",
    "\n",
    "layer_norm = LayerNormalization(axis=-1)\n",
    "layer_norm_out = layer_norm(addition)\n",
    "\n",
    "decoder_output_dense = Dense(VOCAB_SIZE\n",
    ")\n",
    "decoder_output = decoder_output_dense(layer_norm_out)\n",
    "\n",
    "decoder = tf.keras.Model(\n",
    "    inputs=[word_input, encoder_output], outputs=decoder_output\n",
    ")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb97f5c-e65d-4a83-beec-0f80ad0775ea",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b274c8-6155-479f-b84d-72c9811353e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================== Training Model =====================\n",
    "image_caption_train_model = tf.keras.Model(\n",
    "    inputs=[image_input, word_input], outputs=decoder_output\n",
    ")\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction=\"none\"\n",
    ")\n",
    "\n",
    "# @tf.function\n",
    "def loss_function(real, pred):\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    mask = tf.cast(mask, dtype=tf.int32)\n",
    "    sentence_len = tf.reduce_sum(mask)\n",
    "    loss_ = loss_[:sentence_len]\n",
    "    return tf.reduce_mean(loss_, 1)\n",
    "\n",
    "image_caption_train_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=loss_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b5ebbe-ed1b-4d9d-bbcc-ebd704a695aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================== Training Loop =====================\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "history = image_caption_train_model.fit(batched_ds, epochs=1)\n",
    "\n",
    "# Record end time and calculate duration\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "# Convert to hours, minutes, seconds\n",
    "duration = timedelta(seconds=total_time)\n",
    "hours, remainder = divmod(duration.seconds, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "print(f\"\\nTraining completed in: {hours} hours, {minutes} minutes, {seconds} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074f806f-e78d-412d-b505-a116e86e96a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_state_input = Input(shape=(ATTENTION_DIM,), name=\"gru_state_input\")\n",
    "\n",
    "# Reuse trained GRU, but update it so that it can receive states.\n",
    "gru_output, gru_state = decoder_gru(embed_x, initial_state=gru_state_input)\n",
    "\n",
    "# Reuse other layers as well\n",
    "context_vector = decoder_attention([gru_output, encoder_output])\n",
    "addition_output = Add()([gru_output, context_vector])\n",
    "layer_norm_output = layer_norm(addition_output)\n",
    "\n",
    "decoder_output = decoder_output_dense(layer_norm_output)\n",
    "\n",
    "# Define prediction Model with state input and output\n",
    "decoder_pred_model = tf.keras.Model(\n",
    "    inputs=[word_input, gru_state_input, encoder_output],\n",
    "    outputs=[decoder_output, gru_state],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c87b19b-d942-435f-a6f3-6d1ce295f590",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    final_save_dir = './Epochs10'\n",
    "    os.makedirs(final_save_dir, exist_ok=True)\n",
    "\n",
    "    print(\"Saving model weights...\")\n",
    "    encoder.save_weights(os.path.join(final_save_dir, 'encoder.weights.h5'))\n",
    "    decoder.save_weights(os.path.join(final_save_dir, 'decoder.weights.h5'))\n",
    "    decoder_pred_model.save_weights(os.path.join(final_save_dir, 'decoder_pred.weights.h5'))\n",
    "    image_caption_train_model.save_weights(os.path.join(final_save_dir, 'train_model.weights.h5'))\n",
    "\n",
    "    print(\"Saving tokenizer vocabulary...\")\n",
    "    tokenizer_vocab = tokenizer.get_vocabulary()\n",
    "    with open(os.path.join(final_save_dir, 'tokenizer_vocab.json'), 'w') as f:\n",
    "        json.dump(tokenizer_vocab, f)\n",
    "\n",
    "    print(\"Saving model configuration...\")\n",
    "    model_config = {\n",
    "        'IMG_HEIGHT': IMG_HEIGHT,\n",
    "        'IMG_WIDTH': IMG_WIDTH,\n",
    "        'IMG_CHANNELS': IMG_CHANNELS,\n",
    "        'ATTENTION_DIM': ATTENTION_DIM,\n",
    "        'VOCAB_SIZE': VOCAB_SIZE,\n",
    "        'MAX_CAPTION_LEN': MAX_CAPTION_LEN,\n",
    "        'WORD_EMBEDDING_DIM': WORD_EMBEDDING_DIM,\n",
    "        'FEATURES_SHAPE': FEATURES_SHAPE,\n",
    "        'BUFFER_SIZE': BUFFER_SIZE,\n",
    "        'BATCH_SIZE': BATCH_SIZE,\n",
    "    }\n",
    "\n",
    "    # Save training state\n",
    "    training_state = {\n",
    "        'last_epoch': len(history.history['loss']),  # Number of epochs trained\n",
    "        'last_loss': history.history['loss'][-1]     # Last loss value\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(final_save_dir, 'model_config.json'), 'w') as f:\n",
    "        json.dump(model_config, f)\n",
    "\n",
    "    with open(os.path.join(final_save_dir, 'training_state.json'), 'w') as f:\n",
    "        json.dump(training_state, f)\n",
    "\n",
    "    print(\"\\nModel saving completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in model saving: {str(e)}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408e48f1-11b1-46b3-b97c-1240ae2e6512",
   "metadata": {},
   "source": [
    "## Test the model by loading saved weights and configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587cac0b-72dc-4f67-96d2-40cb9c05d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration files and create models\n",
    "def load_models(model_dir='./Epochs10'):\n",
    "    # Load configuration\n",
    "    with open(os.path.join(model_dir, 'model_config.json'), 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # Load vocabulary\n",
    "    with open(os.path.join(model_dir, 'tokenizer_vocab.json'), 'r') as f:\n",
    "        vocab = json.load(f)\n",
    "    \n",
    "    # Create tokenizer\n",
    "    tokenizer = TextVectorization(\n",
    "        max_tokens=config['VOCAB_SIZE'],\n",
    "        output_sequence_length=config['MAX_CAPTION_LEN']\n",
    "    )\n",
    "    tokenizer.set_vocabulary(vocab)\n",
    "    \n",
    "    # Create feature extractor\n",
    "    feature_extractor = tf.keras.applications.inception_resnet_v2.InceptionResNetV2(\n",
    "        include_top=False, weights=\"imagenet\"\n",
    "    )\n",
    "    feature_extractor.trainable = False\n",
    "    \n",
    "    # Create encoder\n",
    "    image_input = Input(shape=(config['IMG_HEIGHT'], config['IMG_WIDTH'], config['IMG_CHANNELS']))\n",
    "    image_features = feature_extractor(image_input)\n",
    "    x = Reshape((8 * 8, 1536))(image_features)\n",
    "    encoder_output = Dense(config['ATTENTION_DIM'], activation=\"relu\")(x)\n",
    "    encoder = tf.keras.Model(inputs=image_input, outputs=encoder_output)\n",
    "    \n",
    "    # Create decoder\n",
    "    word_input = Input(shape=(None,), name=\"words\")\n",
    "    gru_state_input = Input(shape=(config['ATTENTION_DIM'],), name=\"gru_state_input\")\n",
    "    encoder_output_input = Input(shape=(None, config['ATTENTION_DIM']))\n",
    "    \n",
    "    embed_x = Embedding(config['VOCAB_SIZE'], config['ATTENTION_DIM'])(word_input)\n",
    "    decoder_gru = GRU(\n",
    "        config['ATTENTION_DIM'],\n",
    "        return_sequences=True,\n",
    "        return_state=True\n",
    "    )\n",
    "    decoder_attention = Attention()\n",
    "    layer_norm = LayerNormalization(axis=-1)\n",
    "    decoder_output_dense = Dense(config['VOCAB_SIZE'])\n",
    "    \n",
    "    gru_output, gru_state = decoder_gru(embed_x, initial_state=gru_state_input)\n",
    "    context_vector = decoder_attention([gru_output, encoder_output_input])\n",
    "    addition_output = Add()([gru_output, context_vector])\n",
    "    layer_norm_output = layer_norm(addition_output)\n",
    "    decoder_output = decoder_output_dense(layer_norm_output)\n",
    "    \n",
    "    decoder = tf.keras.Model(\n",
    "        inputs=[word_input, gru_state_input, encoder_output_input],\n",
    "        outputs=[decoder_output, gru_state]\n",
    "    )\n",
    "    \n",
    "    # Load weights\n",
    "    encoder.load_weights(os.path.join(model_dir, 'encoder.weights.h5'))\n",
    "    decoder.load_weights(os.path.join(model_dir, 'decoder.weights.h5'))\n",
    "    \n",
    "    return encoder, decoder, tokenizer, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594bc1c1-dd95-47f9-a45e-22d75897f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define beam search prediction\n",
    "def beam_search(image_features, decoder, tokenizer, config, beam_width=3):\n",
    "    vocab = tokenizer.get_vocabulary()\n",
    "    word_to_index = StringLookup(mask_token=\"\", vocabulary=vocab)\n",
    "    \n",
    "    # Initialize with proper shape (1, ATTENTION_DIM)\n",
    "    gru_state = tf.zeros((1, config['ATTENTION_DIM']))\n",
    "    start_token = word_to_index(\"<start>\")\n",
    "    \n",
    "    initial_sequences = [([start_token], 0.0, gru_state)]\n",
    "    completed_sequences = []\n",
    "    \n",
    "    for _ in range(config['MAX_CAPTION_LEN']):\n",
    "        candidates = []\n",
    "        \n",
    "        for seq, score, curr_state in initial_sequences:\n",
    "            if seq[-1] == word_to_index(\"<end>\"):\n",
    "                completed_sequences.append((seq, score))\n",
    "                continue\n",
    "            \n",
    "            # Ensure proper shapes for decoder input\n",
    "            dec_input = tf.expand_dims([seq[-1]], 0)  # Shape: (1, 1)\n",
    "            curr_state = tf.reshape(curr_state, (1, config['ATTENTION_DIM']))  # Ensure proper state shape\n",
    "            \n",
    "            predictions, new_state = decoder(\n",
    "                [dec_input, curr_state, image_features]\n",
    "            )\n",
    "            \n",
    "            logits = predictions[0, 0]\n",
    "            top_k_logits, top_k_indices = tf.math.top_k(logits, k=beam_width)\n",
    "            \n",
    "            for i in range(beam_width):\n",
    "                candidate_seq = seq + [top_k_indices[i].numpy()]\n",
    "                candidate_score = score - tf.math.log(tf.nn.softmax(logits)[top_k_indices[i]])\n",
    "                candidates.append((candidate_seq, candidate_score.numpy(), new_state))\n",
    "        \n",
    "        ordered = sorted(candidates, key=lambda x: x[1])\n",
    "        initial_sequences = ordered[:beam_width]\n",
    "    \n",
    "    if completed_sequences:\n",
    "        best_seq = sorted(completed_sequences, key=lambda x: x[1])[0][0]\n",
    "    else:\n",
    "        best_seq = initial_sequences[0][0]\n",
    "    \n",
    "    return [vocab[idx] for idx in best_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811917d1-cdbd-4629-b2a8-9a772c55f8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define caption prediction and display function\n",
    "def predict_and_display_caption(image_path, encoder, decoder, tokenizer, config):\n",
    "    try:\n",
    "        # Process image\n",
    "        img = tf.image.decode_jpeg(tf.io.read_file(image_path), channels=config['IMG_CHANNELS'])\n",
    "        img = tf.image.resize(img, (config['IMG_HEIGHT'], config['IMG_WIDTH']))\n",
    "        img = tf.keras.applications.inception_resnet_v2.preprocess_input(img)\n",
    "        features = encoder(tf.expand_dims(img, axis=0))\n",
    "        \n",
    "        # Generate caption using beam search\n",
    "        caption_tokens = beam_search(features, decoder, tokenizer, config)\n",
    "        caption = \" \".join([word for word in caption_tokens[1:-1] if word not in [\"<start>\", \"<end>\"]])\n",
    "        caption = caption + \".\"\n",
    "\n",
    "        # Display image and caption\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        image = Image.open(image_path)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Generated Caption:\\n{caption}\", pad=20, wrap=True, fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in prediction: {str(e)}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6e2d4e-c1c1-48ef-8ded-ed6620f8fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "encoder, decoder, tokenizer, config = load_models()\n",
    "\n",
    "# Test on an image\n",
    "image_path = \"images/dog1.jpeg\"\n",
    "predict_and_display_caption(image_path, encoder, decoder, tokenizer, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
